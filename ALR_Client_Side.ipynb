{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matlabData = sio.loadmat(file_name='s2_sl2p_weiss_or_prosail_inout.mat', variable_names=['Input', 'Output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                # B1  443nm, Ultra blue, 60m res\n",
    "                                                                # B2  490nm, blue, 10m res\n",
    "B3 = pandas.DataFrame(matlabData['Input']['Rho_Toc'][0][0])[0]      # 560nm, green, 10m res\n",
    "B4 = pandas.DataFrame(matlabData['Input']['Rho_Toc'][0][0])[1]      # 665nm, red, 10m res\n",
    "B5 = pandas.DataFrame(matlabData['Input']['Rho_Toc'][0][0])[2]      # 705nm, VNIR, 20m res\n",
    "B6 = pandas.DataFrame(matlabData['Input']['Rho_Toc'][0][0])[3]      # 740nm, VNIR, 20m res\n",
    "B7 = pandas.DataFrame(matlabData['Input']['Rho_Toc'][0][0])[4]      # 783nm, VNIR, 20m res\n",
    "                                                                # B8  842nm, VNIR, 10m res\n",
    "B8A = pandas.DataFrame(matlabData['Input']['Rho_Toc'][0][0])[5]     # 865nm, VNIR, 20m res\n",
    "                                                                # B9  940nm, SWIR, 60m res\n",
    "                                                                # B10 1375nm, SWIR, 60m res\n",
    "B11 = pandas.DataFrame(matlabData['Input']['Rho_Toc'][0][0])[6]     # 1610nm, SWIR, 20m res\n",
    "B12 = pandas.DataFrame(matlabData['Input']['Rho_Toc'][0][0])[7]     # 2190nm, SWIR, 20m res\n",
    "\n",
    "LAI = pandas.Series(data=matlabData['Output']['LAI'][0][0].flatten())\n",
    "FAPAR = pandas.Series(data=matlabData['Output']['FAPAR'][0][0].flatten())\n",
    "FCOVER = pandas.Series(data=matlabData['Output']['FCOVER'][0][0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDVI = (B8A-B4)/(B8A+B4)\n",
    "RVI1 = B4/B8A\n",
    "RVI2 = B4/B7\n",
    "RVI3 = B4/B6\n",
    "RVI4 = B4/B5\n",
    "DVI1 = B8A-B4\n",
    "DVI2 = B7-B4\n",
    "DVI3 = B6-B4\n",
    "DVI4 = B5-B4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAImodel = sklearn.linear_model.Lars(n_nonzero_coefs=11)\n",
    "LAImodel = LAImodel.fit(inputDF, LAI)\n",
    "\n",
    "FAPARmodel = sklearn.linear_model.Lars(n_nonzero_coefs=11)\n",
    "FAPARmodel = FAPARmodel.fit(inputDF, FAPAR)\n",
    "\n",
    "FCOVERmodel = sklearn.linear_model.Lars(n_nonzero_coefs=11)\n",
    "FCOVERmodel = FCOVERmodel.fit(inputDF, FCOVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_RMSE = sklearn.metrics.mean_squared_error(LAI, LAI_predicted, squared=False)\n",
    "FAPAR_RMSE = sklearn.metrics.mean_squared_error(FAPAR, FAPAR_predicted, squared=False)\n",
    "FCOVER_RMSE = sklearn.metrics.mean_squared_error(FCOVER, FCOVER_predicted, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LAI Coefficient Path - RMSE: {}'.format(LAI_RMSE))\n",
    "pandas.DataFrame(LAImodel.coef_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FAPAR Coefficient Path - RMSE: {}'.format(FAPAR_RMSE))\n",
    "pandas.DataFrame(FAPARmodel.coef_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FCOVER Coefficient Path - RMSE: {}'.format(FCOVER_RMSE))\n",
    "pandas.DataFrame(FCOVERmodel.coef_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAImodel = sklearn.linear_model.Lars(n_nonzero_coefs=4)\n",
    "LAImodel = LAImodel.fit(inputDF, LAI)\n",
    "\n",
    "FAPARmodel = sklearn.linear_model.Lars(n_nonzero_coefs=3)\n",
    "FAPARmodel = FAPARmodel.fit(inputDF, FAPAR)\n",
    "\n",
    "FCOVERmodel = sklearn.linear_model.Lars(n_nonzero_coefs=3)\n",
    "FCOVERmodel = FCOVERmodel.fit(inputDF, FCOVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_predicted = pandas.Series(LAImodel.predict(inputDF))\n",
    "FAPAR_predicted = pandas.Series(FAPARmodel.predict(inputDF))\n",
    "FCOVER_predicted = pandas.Series(FCOVERmodel.predict(inputDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_features = numpy.nonzero(LAImodel.coef_)[0]\n",
    "FAPAR_features = numpy.nonzero(FAPARmodel.coef_)[0]\n",
    "FCOVER_features = numpy.nonzero(FCOVERmodel.coef_)[0]\n",
    "\n",
    "LAI_features = inputDF.columns[LAI_features]\n",
    "FAPAR_features = inputDF.columns[FAPAR_features]\n",
    "FCOVER_features = inputDF.columns[FCOVER_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.sigmoid, \n",
    "                                  input_shape=[len(inputDF[LAI_features].keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.sigmoid),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "LAI_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "\n",
    "FAPAR_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.sigmoid, \n",
    "                                  input_shape=[len(inputDF[FAPAR_features].keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.sigmoid),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "FAPAR_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])\n",
    "\n",
    "\n",
    "FCOVER_model = tensorflow.keras.models.Sequential([\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.sigmoid, \n",
    "                                  input_shape=[len(inputDF[FCOVER_features].keys())]),\n",
    "    tensorflow.keras.layers.Dense(10, activation=tensorflow.nn.sigmoid),\n",
    "    tensorflow.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "FCOVER_model.compile(\n",
    "    optimizer=tensorflow.keras.optimizers.Nadam(),\n",
    "    loss='mse',\n",
    "    metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_model.fit(x = inputDF[LAI_features], y = LAI, epochs = 100, validation_split = 0.2)\n",
    "FAPAR_model.fit(x = inputDF[FAPAR_features], y = FAPAR, epochs = 20, validation_split = 0.2)\n",
    "FCOVER_model.fit(x = inputDF[FCOVER_features], y = FCOVER, epochs = 20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_predictions = pandas.Series(LAI_model.predict(inputDF[LAI_features]).flatten())\n",
    "FAPAR_predictions = pandas.Series(FAPAR_model.predict(inputDF[FAPAR_features]).flatten())\n",
    "FCOVER_predictions = pandas.Series(FCOVER_model.predict(inputDF[FCOVER_features]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_skl_LAI = numpy.vstack([LAI, LAI_predicted])\n",
    "xy_tf_LAI = numpy.vstack([LAI, LAI_predictions])\n",
    "xy_skl_FAPAR = numpy.vstack([FAPAR, FAPAR_predicted])\n",
    "xy_tf_FAPAR = numpy.vstack([FAPAR, FAPAR_predictions])\n",
    "xy_skl_FCOVER = numpy.vstack([FCOVER, FCOVER_predicted])\n",
    "xy_tf_FCOVER = numpy.vstack([FCOVER, FCOVER_predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = scipy.stats.gaussian_kde(xy_skl_LAI)(xy_skl_LAI)\n",
    "z_tf_LAI = scipy.stats.gaussian_kde(xy_tf_LAI)(xy_tf_LAI)\n",
    "z_skl_FAPAR = scipy.stats.gaussian_kde(xy_skl_FAPAR)(xy_skl_FAPAR)\n",
    "z_tf_FAPAR = scipy.stats.gaussian_kde(xy_tf_FAPAR)(xy_tf_FAPAR)\n",
    "z_skl_FCOVER = scipy.stats.gaussian_kde(xy_skl_FCOVER)(xy_skl_FCOVER)\n",
    "z_tf_FCOVER = scipy.stats.gaussian_kde(xy_tf_FCOVER)(xy_tf_FCOVER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_skl_LAI = z_skl_LAI.argsort()\n",
    "idx_tf_LAI = z_tf_LAI.argsort()\n",
    "idx_skl_FAPAR = z_skl_FAPAR.argsort()\n",
    "idx_tf_FAPAR = z_tf_FAPAR.argsort()\n",
    "idx_skl_FCOVER = z_skl_FCOVER.argsort()\n",
    "idx_tf_FCOVER = z_tf_FCOVER.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_skl_LAI = LAI[idx_skl_LAI]\n",
    "x_tf_LAI = LAI[idx_tf_LAI]\n",
    "x_skl_FAPAR = FAPAR[idx_skl_FAPAR]\n",
    "x_tf_FAPAR = FAPAR[idx_tf_FAPAR]\n",
    "x_skl_FCOVER = FCOVER[idx_skl_FCOVER]\n",
    "x_tf_FCOVER = FCOVER[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_skl_LAI = LAI_predicted[idx_skl_LAI]\n",
    "y_tf_LAI = LAI_predictions[idx_tf_LAI]\n",
    "y_skl_FAPAR = FAPAR_predicted[idx_skl_FAPAR]\n",
    "y_tf_FAPAR = FAPAR_predictions[idx_tf_FAPAR]\n",
    "y_skl_FCOVER = FCOVER_predicted[idx_skl_FCOVER]\n",
    "y_tf_FCOVER = FCOVER_predictions[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_skl_LAI = z_skl_LAI[idx_skl_LAI]\n",
    "z_tf_LAI = z_tf_LAI[idx_tf_LAI]\n",
    "z_skl_FAPAR = z_skl_FAPAR[idx_skl_FAPAR]\n",
    "z_tf_FAPAR = z_tf_FAPAR[idx_tf_FAPAR]\n",
    "z_skl_FCOVER = z_skl_FCOVER[idx_skl_FCOVER]\n",
    "z_tf_FCOVER = z_tf_FCOVER[idx_tf_FCOVER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_skl_LAI = sklearn.metrics.mean_squared_error(x_skl_LAI, y_skl_LAI, squared=False)\n",
    "rmse_tf_LAI = sklearn.metrics.mean_squared_error(x_tf_LAI, y_tf_LAI, squared=False)\n",
    "rmse_skl_FAPAR = sklearn.metrics.mean_squared_error(x_skl_FAPAR, y_skl_FAPAR, squared=False)\n",
    "rmse_tf_FAPAR = sklearn.metrics.mean_squared_error(x_tf_FAPAR, y_tf_FAPAR, squared=False)\n",
    "rmse_skl_FCOVER = sklearn.metrics.mean_squared_error(x_skl_FCOVER, y_skl_FCOVER, squared=False)\n",
    "rmse_tf_FCOVER = sklearn.metrics.mean_squared_error(x_tf_FCOVER, y_tf_FCOVER, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_skl_LAI = numpy.linspace(0, 6, 1000)\n",
    "a_tf_LAI = numpy.linspace(0, 10, 1000)\n",
    "a_skl_FAPAR = numpy.linspace(0, 1, 1000)\n",
    "a_tf_FAPAR = numpy.linspace(0, 1, 1000)\n",
    "a_skl_FCOVER = numpy.linspace(0, 1, 1000)\n",
    "a_tf_FCOVER = numpy.linspace(0, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 3, figsize=(25,15))\n",
    "\n",
    "\n",
    "ax[0,0].scatter(x_skl_LAI, y_skl_LAI, c=z_skl_LAI)\n",
    "ax[0,0].plot(a_skl_LAI, a_skl_LAI, c='r')\n",
    "ax[0,0].set_title('LASSO LARS LAI - RMSE: {}'.format(rmse_skl_LAI))\n",
    "\n",
    "\n",
    "ax[1,0].scatter(x_tf_LAI, y_tf_LAI, c=z_tf_LAI)\n",
    "ax[1,0].plot(a_tf_LAI, a_tf_LAI, c='r')\n",
    "ax[1,0].set_title('LASSO LARS FAPAR - RMSE: {}'.format(rmse_tf_LAI))\n",
    "\n",
    "\n",
    "ax[0,1].scatter(x_skl_FAPAR, y_skl_FAPAR, c=z_skl_FAPAR)\n",
    "ax[0,1].plot(a_skl_FAPAR, a_skl_FAPAR, c='r')\n",
    "ax[0,1].set_title('LASSO LARS FCOVER - RMSE: {}'.format(rmse_skl_FAPAR))\n",
    "\n",
    "\n",
    "ax[1,1].scatter(x_tf_FAPAR, y_tf_FAPAR, c=z_tf_FAPAR)\n",
    "ax[1,1].plot(a_tf_FAPAR, a_tf_FAPAR, c='r')\n",
    "ax[1,1].set_title('NNet LAI - RMSE: {}'.format(rmse_tf_FAPAR))\n",
    "\n",
    "\n",
    "ax[0,2].scatter(x_skl_FCOVER, y_skl_FCOVER, c=z_skl_FCOVER)\n",
    "ax[0,2].plot(a_skl_FCOVER, a_skl_FCOVER, c='r')\n",
    "ax[0,2].set_title('NNet FAPAR - RMSE: {}'.format(rmse_skl_FCOVER))\n",
    "\n",
    "\n",
    "ax[1,2].scatter(x_tf_FCOVER, y_tf_FCOVER, c=z_tf_FCOVER)\n",
    "ax[1,2].plot(a_tf_FCOVER, a_tf_FCOVER, c='r')\n",
    "ax[1,2].set_title('NNet FCOVER - RMSE: {}'.format(rmse_tf_FCOVER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EE_LARS_Regression(assetName, features, response, maxSamples, n_nonzero):\n",
    "        \n",
    "    inputCSV = ee.FeatureCollection(assetName)\n",
    "    inputCSV = inputCSV.toList(count=maxSamples)\n",
    "    \n",
    "    def extractBands(feature):\n",
    "        feature = ee.Feature(feature)\n",
    "        return feature.toArray(properties=features).toList()\n",
    "\n",
    "    def extractVI(feature):\n",
    "        feature = ee.Feature(feature)\n",
    "        return feature.toArray(properties=[response]).toList()\n",
    "    \n",
    "    inputList = inputCSV.map(extractBands)\n",
    "    outputList = inputCSV.map(extractVI)\n",
    "    \n",
    "    X = ee.Array(inputList)\n",
    "    y = ee.Array(outputList)\n",
    "    \n",
    "    n = X.length().get([0])\n",
    "    m = X.length().get([1])\n",
    "    \n",
    "    \n",
    "    def centre(output):\n",
    "        output = ee.Array(output)\n",
    "        mean = output.reduce(ee.Reducer.mean(), [0]).get([0,0])\n",
    "        return output.subtract(mean)\n",
    "        \n",
    "    def normalize(inputs):\n",
    "        inputs = ee.Array(inputs)\n",
    "        \n",
    "        inputMeans = inputs.reduce(ee.Reducer.mean(), [0])\n",
    "        inputMeans = inputMeans.repeat(0, n)\n",
    "        inputs = inputs.subtract(inputMeans)\n",
    "        inputs = inputs.pow(2).reduce(ee.Reducer.sum(), [0]).pow(-0.5).repeat(0,n).multiply(inputs)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    X = normalize(X)\n",
    "    y = centre(y)\n",
    "    \n",
    "    def LARSregression(iteration, inputs):\n",
    "        inputs = ee.Dictionary(inputs)\n",
    "        prediction = inputs.getArray('prediction')\n",
    "        coeff_arr = inputs.getArray('coeff_arr')\n",
    "    \n",
    "        c = X.matrixTranspose().matrixMultiply(y.subtract(prediction))\n",
    "        c_abs = c.abs()\n",
    "        C_max = c_abs.get(c_abs.argmax())\n",
    "\n",
    "        maxLocs = c_abs.gte(C_max.subtract(0.00001))\n",
    "        signs = c.divide(c_abs)\n",
    "\n",
    "        signs_j = maxLocs.multiply(signs).matrixTranspose()\n",
    "        signs_jc = signs_j.abs().subtract(1).multiply(-1)\n",
    "        \n",
    "        A = ee.List(ee.Array([ee.List.sequence(0, m.subtract(1))]).mask(signs_j).toList().get(0))\n",
    "        A_c = ee.List(ee.Array([ee.List.sequence(0, m.subtract(1))]).mask(signs_jc).toList().get(0))\n",
    "\n",
    "        signMatrix_j = signs_j.repeat(0, n)\n",
    "\n",
    "        X_A = X.multiply(signMatrix_j).mask(signs_j)\n",
    "        j = X_A.length().get([1])\n",
    "        \n",
    "        G_A = X_A.matrixTranspose().matrixMultiply(X_A)\n",
    "\n",
    "        V1_A = ee.Array(ee.List.repeat([1], j))\n",
    "\n",
    "        G_Ai = G_A.matrixInverse()\n",
    "\n",
    "        A_A = V1_A.matrixTranspose().matrixMultiply(G_Ai).matrixMultiply(V1_A).get([0,0]).pow(-0.5)\n",
    "\n",
    "        w_A = G_Ai.matrixMultiply(V1_A).multiply(A_A)\n",
    "\n",
    "        u_A = X_A.matrixMultiply(w_A)\n",
    "\n",
    "        a = X.matrixTranspose().matrixMultiply(u_A)\n",
    "\n",
    "        def computeGammaRRay(index_j):\n",
    "            minus_j = C_max.subtract(c.get([index_j, 0])).divide(A_A.subtract(a.get([index_j, 0])))\n",
    "            plus_j = C_max.add(c.get([index_j, 0])).divide(A_A.add(a.get([index_j, 0])))\n",
    "\n",
    "            gammaRRay = ee.Array([minus_j, plus_j]);\n",
    "            gammaRRay = gammaRRay.mask(gammaRRay.gte(0))\n",
    "            gammaRRay = gammaRRay.multiply(-1)\n",
    "\n",
    "            return gammaRRay.get(gammaRRay.argmax())\n",
    "\n",
    "        gammaRRay = ee.Array([A_c.map(computeGammaRRay)])\n",
    "        gamma = gammaRRay.get(gammaRRay.argmax()).multiply(-1)\n",
    "\n",
    "        prediction = prediction.add(u_A.multiply(gamma))\n",
    "        coefficients = X.matrixSolve(prediction)\n",
    "\n",
    "        def setZero(num):\n",
    "            num = ee.Number(num)\n",
    "            return ee.Algorithms.If(num.abs().lt(0.0000000001), [0], [num])\n",
    "\n",
    "        coefficients = ee.Array(ee.List(coefficients.matrixTranspose().toList().get(0)).map(setZero))\n",
    "\n",
    "        coeff_arr = ee.Array.cat([coeff_arr, coefficients], axis=1)\n",
    "\n",
    "        outputs = ee.Dictionary({'prediction':prediction, 'coeff_arr':coeff_arr})\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    numIterations = ee.List.sequence(1, n_nonzero)\n",
    "    prediction = ee.Array(ee.List.repeat([0], n))\n",
    "    coeff_arr = ee.Array(ee.List.repeat([0], m))\n",
    "    initial = ee.Dictionary({'prediction':prediction, 'coeff_arr':coeff_arr})\n",
    "\n",
    "    finalOutputs = numIterations.iterate(LARSregression, initial)\n",
    "    finalOutputs = ee.Dictionary(finalOutputs)\n",
    "    finalPrediction = finalOutputs.getArray('prediction')\n",
    "\n",
    "    coeff_arr = finalOutputs.getArray('coeff_arr')\n",
    "    coeff_arr = coeff_arr.getInfo()\n",
    "    coeff_arr = numpy.asarray(coeff_arr)\n",
    "    \n",
    "    return coeff_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_coef = EE_LARS_Regression('users/ccrs2fy2020/rawFeatures', \n",
    "                              ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3'], \n",
    "                              'LAI', \n",
    "                              50000, \n",
    "                              10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_coef = EE_LARS_Regression('users/ccrs2fy2020/rawFeatures',\n",
    "                                ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3'],\n",
    "                                'FAPAR', \n",
    "                                50000, \n",
    "                                10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_coef = EE_LARS_Regression('users/ccrs2fy2020/rawFeatures', \n",
    "                                 ['B0', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'A1', 'A2', 'A3'],\n",
    "                                 'FCOVER', \n",
    "                                 50000, \n",
    "                                 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_coef = pandas.DataFrame(LAI_coef)\n",
    "FAPAR_coef = pandas.DataFrame(FAPAR_coef)\n",
    "FCOVER_coef = pandas.DataFrame(FCOVER_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAI_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAPAR_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FCOVER_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LAI_FAPAR_FCOVER_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}